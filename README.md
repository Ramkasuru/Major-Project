# VIOLA: Video Integration of Object Detection, Language Insights, and Accessibility  

### ğŸ¯ Overview  
**VIOLA** is a real-time video analysis framework that integrates **YOLOv8** (object detection), **Whisper/Google STT** (speech-to-text), and **GPT-4** (contextual insights) into a unified pipeline. The system enhances **accessibility, comprehension, and situational awareness** for live video streams by combining vision, audio, and language-based intelligence.  

---

### ğŸ› ï¸ Tech Stack  
- **Object Detection** â†’ YOLOv8  
- **Speech-to-Text** â†’ OpenAI Whisper, Google Speech-to-Text  
- **Text-to-Speech** â†’ Pyttsx3  
- **Language Insights** â†’ GPT-4  
- **Interface** â†’ Custom user-friendly UI for live visualization  

---

### ğŸ“Œ Key Features  
- Real-time **object detection** with YOLOv8.  
- **Speech transcription** via Whisper/Google STT.  
- **Contextual analysis & insights** with GPT-4.  
- **Accessibility focus** with text-to-speech and enriched video overlays.  
- Modular design for **surveillance, education, assistive tech, and immersive UX**.  

---

### ğŸ”¬ Methodology  
1. **Capture live video** (camera/webcam).  
2. **YOLOv8 detects objects** within frames.  
3. **Whisper/Google STT transcribes speech** in real time.  
4. **GPT-4 generates contextual insights** (summaries, sentiment, entity recognition).  
5. **User interface overlays** objects, transcripts, and insights on live video.  

---

### ğŸ“Š Results  
- Accurate **real-time detection** of objects and speech transcription.  
- Successful **fusion of multi-modal insights** into a single visual display.  
- Demonstrated potential in **accessibility applications** (live captioning, assistive video tools).  

---

### ğŸš€ Future Work  
- Multimodal fusion (video + audio + text at deeper levels).  
- Continual learning for long-term adaptability.  
- Edge optimization for mobile/IoT deployment.  
- Privacy-preserving and federated learning approaches.  

---

### ğŸ† Recognition  
- Published as *â€œVIOLA: Video Integration of Object Detection, Language Insights and Accessibilityâ€* in the 2024 International Student Conference.  
- Received awards for **Best AI for Social Impact**.  

---

### âš¡ Quick Start  

#### 1. Clone the repository  
git clone https://github.com/yourusername/viola.git  
cd viola  

#### 2. Create a virtual environment (recommended)  
python -m venv venv  
source venv/bin/activate   # On Linux/Mac  
venv\Scripts\activate      # On Windows  

#### 3. Install dependencies  
pip install -r requirements.txt  

#### 4. Run the application  
python main.py  

#### 5. Expected workflow  
- A window will open showing live **video feed**.  
- Objects will be detected and highlighted via **YOLOv8**.  
- Speech will be transcribed in **real time** using Whisper/Google STT.  
- **GPT-4 insights** will be generated and displayed alongside the video.  
- Optionally, **TTS (pyttsx3)** will read insights aloud for accessibility.  

---

### ğŸ“‚ Repository Structure  
â”œâ”€â”€ /src  
â”‚   â”œâ”€â”€ object_detection/      # YOLOv8 implementation  
â”‚   â”œâ”€â”€ speech_to_text/        # Whisper + Google STT  
â”‚   â”œâ”€â”€ text_to_speech/        # Pyttsx3 module  
â”‚   â”œâ”€â”€ insights/              # GPT-4 contextual analysis  
â”‚   â””â”€â”€ ui/                    # User interface  
â”œâ”€â”€ /data                      # Sample test data  
â”œâ”€â”€ main.py                    # Entry point  
â”œâ”€â”€ requirements.txt           # Dependencies  
â””â”€â”€ README.md                  # Documentation  

---

### ğŸ¤ Contributors  
- **Ram Kasuru** â€“ Computer Science (AI & ML), Dayananda Sagar University  
- Jeramiah T Varghese, C S Jeevan, Hridanshu Ruparel  
